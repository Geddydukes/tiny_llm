Metadata-Version: 2.4
Name: tiny-llm
Version: 0.1.0
Summary: Tiny yet modern language model stack with tokenizer, data prep, and training scripts.
Author: tiny-llm
License: MIT
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: torch>=2.2
Requires-Dist: numpy>=1.26
Requires-Dist: tqdm>=4.66
Requires-Dist: sentencepiece>=0.1.99
Provides-Extra: dev
Requires-Dist: ruff>=0.6; extra == "dev"
Requires-Dist: pytest>=8.1; extra == "dev"

# tiny-llm

Minimal yet production-grade tiny language model stack. Includes:

- SentencePiece tokenizer tooling with chat-friendly special tokens
- Data pipeline that shards token sequences into `.npy` blocks
- PyTorch Transformer with RoPE, RMSNorm, SwiGLU, weight tying
- Training and sanity-check scripts tuned for CPU/MPS/GPU on macOS

## Quickstart

1. **Train tokenizer**
   `python scripts/train_tokenizer.py --input data.txt --model_prefix tokenizer`
2. **Prepare data**
   `python scripts/prepare_data.py --tokenizer tokenizer.model --input data.txt --output_dir data/shards`
3. **Pretrain model**
   `python scripts/train_pretrain.py --tokenizer tokenizer.model --data_dir data/shards --out_dir checkpoints`
4. **Sanity check**
   `python scripts/sanity_check_model.py`

All code targets Python 3.10+.
